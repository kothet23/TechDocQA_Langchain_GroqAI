import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
from sentence_transformers import SentenceTransformer, util
from langchain_groq import ChatGroq
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os
import time
import traceback

# ===========================
# PAGE CONFIGURATION
# ===========================
st.set_page_config(
    page_title="Smart Web Q&A System",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .stAlert {
        margin-top: 1rem;
    }
    .source-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ===========================
# SESSION STATE INITIALIZATION
# ===========================
if 'faiss_index' not in st.session_state:
    st.session_state.faiss_index = None
if 'selected_urls' not in st.session_state:
    st.session_state.selected_urls = []
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False

# ===========================
# UTILITY FUNCTIONS
# ===========================

@st.cache_data(show_spinner=False)
def get_sub_urls_with_metadata(base_url, timeout=10):
    """Extract all links from the base URL with metadata"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(base_url, headers=headers, timeout=timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        results = []
        base_domain = urlparse(base_url).netloc
        
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            full_url = urljoin(base_url, href)
            
            # Filter to same domain and valid URLs
            if full_url.startswith("http") and urlparse(full_url).netloc == base_domain:
                anchor_text = a_tag.get_text(strip=True)
                parsed_url = urlparse(full_url)
                last_part = unquote(parsed_url.path.rstrip("/").split("/")[-1])
                
                # Create descriptive text
                full_text = anchor_text if anchor_text else last_part
                if last_part and anchor_text and last_part.lower() not in anchor_text.lower():
                    full_text += f" ({last_part})"
                
                if full_text:  # Only add if there's meaningful text
                    results.append({"url": full_url, "text": full_text})
        
        # Remove duplicates
        seen = set()
        unique_results = []
        for item in results:
            if item["url"] not in seen:
                seen.add(item["url"])
                unique_results.append(item)
        
        return unique_results
    except Exception as e:
        st.error(f"Error fetching URL: {str(e)}")
        return []


@st.cache_resource
def load_embedding_model():
    """Load sentence transformer model"""
    with st.spinner("Loading embedding model..."):
        return SentenceTransformer('all-MiniLM-L6-v2')


def find_relevant_links(sublinks, question, model, k=3):
    """Find most relevant links using semantic similarity"""
    if not sublinks:
        return []
    
    try:
        link_texts = [item["text"] for item in sublinks]
        link_embeddings = model.encode(link_texts, convert_to_tensor=True)
        query_embedding = model.encode(question, convert_to_tensor=True)
        
        cos_scores = util.pytorch_cos_sim(query_embedding, link_embeddings)[0]
        top_k = min(k, len(sublinks))
        top_indices = cos_scores.topk(k=top_k).indices
        
        selected_links = [sublinks[idx] for idx in top_indices]
        return selected_links
    except Exception as e:
        st.error(f"Error finding relevant links: {str(e)}")
        return []


def create_vector_store(urls):
    """Create FAISS vector store from URLs"""
    try:
        with st.spinner("Loading web content..."):
            loader = WebBaseLoader(urls)
            loader.requests_kwargs = {'verify': True, 'timeout': 10}
            documents = loader.load()
            
            if not documents:
                st.error("No content could be loaded from the URLs")
                return None
            
            st.success(f"Loaded {len(documents)} documents")
        
        with st.spinner("Splitting documents into chunks..."):
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=2000,
                chunk_overlap=50,
                length_function=len
            )
            chunks = text_splitter.split_documents(documents)
            st.info(f"Created {len(chunks)} text chunks")
        
        with st.spinner("Creating vector embeddings..."):
            embedding_model = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'}
            )
            faiss_index = FAISS.from_documents(documents=chunks, embedding=embedding_model)
            st.success("Vector store created successfully!")
        
        return faiss_index, embedding_model, documents
        
    except Exception as e:
        st.error(f"Error creating vector store: {str(e)}")
        st.code(traceback.format_exc())
        return None


def get_answer(question, faiss_index, llm):
    """Get answer using RAG chain"""
    try:
        # Create prompt template
        prompt_template = """Use the following pieces of context to answer the question at the end. 
        If you don't know the answer, just say that you don't know, don't try to make up an answer.
        Provide a detailed and comprehensive answer based on the context.

        Context: {context}

        Question: {question}

        Detailed Answer:"""
        
        PROMPT = PromptTemplate(
            template=prompt_template, 
            input_variables=["context", "question"]
        )
        
        # Create retrieval chain
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=faiss_index.as_retriever(search_kwargs={"k": 5}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        result = chain({"query": question})
        return result
    except Exception as e:
        st.error(f"Error getting answer: {str(e)}")
        st.code(traceback.format_exc())
        return None


# ===========================
# MAIN APPLICATION
# ===========================

# Header
st.markdown('<p class="main-header">Smart Web Q&A System</p>', unsafe_allow_html=True)
st.markdown("Ask questions about any website's content with AI-powered semantic search")

# Sidebar Configuration
with st.sidebar:
    st.header("Configuration")
    
    # API Key Input
    st.subheader("API Configuration")
    if not os.getenv("GROQ_API_KEY"):
        groq_api_key = st.text_input(
            "Groq API Key",
            type="password",
            help="Get your API key from https://console.groq.com"
        )
        if groq_api_key:
            os.environ["GROQ_API_KEY"] = groq_api_key
    else:
        st.success("API Key configured")
    
    # Model Selection
    st.subheader("Model Settings")
    model_name = st.selectbox(
        "Select LLM Model",
        ["llama-3.3-70b-versatile", "mixtral-8x7b-32768", "llama-3.1-70b-versatile"],
        index=0
    )
    
    temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.1)
    
    # Search Settings
    st.subheader("Search Settings")
    top_k = st.slider("Number of links to analyze", 1, 10, 3)
    
    st.divider()
    
    # Instructions
    st.subheader("How to Use")
    st.markdown("""
    1. **Enter URL** of the webpage
    2. **Enter question** about the content
    3. **Click 'Process & Answer'**
    4. Review the AI-generated answer
    
    **Tip**: More specific questions yield better answers!
    """)
    
    # Reset button
    if st.button("Reset Session", use_container_width=True):
        st.session_state.faiss_index = None
        st.session_state.selected_urls = []
        st.session_state.processing_complete = False
        st.rerun()

# Main Content Area
col1, col2 = st.columns([3, 1])

with col1:
    url = st.text_input(
        "Website URL",
        placeholder="https://example.com",
        help="Enter the URL you want to analyze"
    )

with col2:
    st.write("")  # Spacing
    st.write("")  # Spacing

question = st.text_area(
    "Your Question",
    placeholder="What would you like to know about this website?",
    height=100,
    help="Ask any question about the content of the website"
)

# Process Button
process_button = st.button("Process & Answer", type="primary", use_container_width=True)

# Main Processing Logic
if process_button:
    if not url or not question:
        st.warning("Please provide both URL and question")
        st.stop()
    
    if not os.getenv("GROQ_API_KEY"):
        st.error("Please provide a Groq API key in the sidebar")
        st.stop()
    
    # Create progress container
    progress_container = st.container()
    
    with progress_container:
        try:
            # Initialize LLM
            llm = ChatGroq(
                model_name=model_name,
                temperature=temperature,
                max_tokens=2048
            )
            
            # Step 1: Extract links
            with st.status("ðŸ”— Extracting links from webpage...", expanded=True) as status:
                sublinks = get_sub_urls_with_metadata(url)
                
                if not sublinks:
                    st.warning("No sublinks found. Analyzing main page only...")
                    selected_urls = [url]
                    selected_links = [{"url": url, "text": "Main page"}]
                else:
                    st.write(f"Found {len(sublinks)} links")
                    
                    # Step 2: Find relevant links
                    st.write("Finding most relevant pages...")
                    model = load_embedding_model()
                    selected_links = find_relevant_links(sublinks, question, model, k=top_k)
                    selected_urls = [link['url'] for link in selected_links]
                    st.write(f"Selected {len(selected_urls)} most relevant pages")
                
                status.update(label="Link extraction complete!", state="complete")
            
            # Step 3: Create vector store
            with st.status("Processing content...", expanded=True) as status:
                result = create_vector_store(selected_urls)
                if result is None:
                    st.stop()
                faiss_index, embedding_model, documents = result
                st.session_state.faiss_index = faiss_index
                st.session_state.selected_urls = selected_urls
                status.update(label="Content indexed!", state="complete")
            
            # Step 4: Generate answer
            with st.status("Generating answer...", expanded=True) as status:
                answer_result = get_answer(question, faiss_index, llm)
                if answer_result:
                    st.write("Answer generated successfully")
                    status.update(label="Complete!", state="complete")
            
            # Display Results
            if answer_result:
                st.divider()
                
                # Answer Section
                st.subheader("Answer")
                st.markdown(f"""
                <div style='background-color: #e8f4f8; padding: 1.5rem; border-radius: 0.5rem; border-left: 5px solid #1f77b4;'>
                    {answer_result['result']}
                </div>
                """, unsafe_allow_html=True)
                
                st.divider()
                
                # Sources Section
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.subheader("Analyzed Pages")
                    for i, link in enumerate(selected_links, 1):
                        st.markdown(f"""
                        <div class='source-card'>
                            <b>{i}.</b> <a href='{link['url']}' target='_blank'>{link['text']}</a>
                        </div>
                        """, unsafe_allow_html=True)
                
                with col2:
                    st.subheader("Statistics")
                    st.metric("Total Documents", len(documents))
                    st.metric("Pages Analyzed", len(selected_urls))
                    st.metric("Source Documents Used", len(answer_result.get('source_documents', [])))
                
                # Source Documents Preview
                with st.expander("View Source Excerpts", expanded=False):
                    for i, doc in enumerate(answer_result.get('source_documents', [])[:3], 1):
                        st.markdown(f"**Source {i}:**")
                        st.text(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                        st.caption(f"From: {doc.metadata.get('source', 'Unknown')}")
                        st.divider()
        
        except Exception as e:
            st.error(f"An unexpected error occurred: {str(e)}")
            with st.expander("View Error Details"):
                st.code(traceback.format_exc())

# Footer
st.divider()
st.caption("Built with Streamlit, LangChain, and Groq AI | Semantic search powered by Sentence Transformers")
